{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f3a0c8",
   "metadata": {},
   "source": [
    "ΙΩΑΝΝΗΣ ΓΙΑΝΝΑΚΟΣ 4970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031bc4c",
   "metadata": {},
   "source": [
    "# Βήμα 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccdb164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Description  \\\n",
      "6997  <b>MICTUNING SAE to USB Cable Adapter 3.1A Dua...   \n",
      "6998  <b>Beeiee S5 Wireless Charging Receiver</b><br...   \n",
      "6999  <b>ONSON Dual USB Car Charger with safety Hamm...   \n",
      "7000  VINTRONS Car Charger, 10.5W/2.1A, with 1 Unive...   \n",
      "7001                               only for freway vr01   \n",
      "7002  <b>Our promise to you</b><br><br> All HAVIT pr...   \n",
      "7003  <b>Fitian Magnetic DC Charging Cable cord for ...   \n",
      "7004  <br>[Apple MFi Certified] 3Pack SEGMOI Lightni...   \n",
      "7005  This travel adapter has 5 built-in plug-ins (i...   \n",
      "7006  <b>Technical Specification</b><br> Model: Y020...   \n",
      "\n",
      "                       Category  \n",
      "6997  Chargers & Power Adapters  \n",
      "6998  Chargers & Power Adapters  \n",
      "6999               Car Chargers  \n",
      "7000               Car Chargers  \n",
      "7001  Chargers & Power Adapters  \n",
      "7002               Car Chargers  \n",
      "7003               Car Chargers  \n",
      "7004               Car Chargers  \n",
      "7005  Chargers & Power Adapters  \n",
      "7006               Car Chargers  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Διάβασμα του αρχείου με τα δεδομένα\n",
    "with open('meta_Cell_Phones_and_Accessories.json') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "valid_data = [] # Αρχικοποίηση λίστας για τα τελικά δεδομένα και το σύνολο για τα μοναδικά asin\n",
    "seen_asins = set()\n",
    "\n",
    "# Ελέγχος κάθε JSON αντικειμένου\n",
    "for line in data:\n",
    "    product = json.loads(line)\n",
    "    \n",
    "    # Ελέγχος αν η κατηγορία του καθε προιόντος είναι έγκυρη && αν η περιγραφή κάθε αντικειμένου είναι κενή\n",
    "    if (\"Chargers & Power Adapters\" in product[\"category\"] or \"Car Chargers\" in product[\"category\"]) and (len(product[\"description\"])>0):\n",
    "        \n",
    "        if \"Chargers & Power Adapters\" in product[\"category\"][-1] or \"Car Chargers\" in product[\"category\"][-1]:\n",
    "            #  έλεγχος αν το asin έχει ήδη προστεθεί, δλδ αν έχω ξαναδεί το συγκεκριμένο προιόν \n",
    "            asin = product[\"asin\"]\n",
    "            if asin in seen_asins:\n",
    "                continue  # αν ναι τότε το προσπερνάω\n",
    "            else:\n",
    "                seen_asins.add(asin)  # αλλιώς το προσθέτω στο σύνολο για μελλοντικό έλεγχο\n",
    "\n",
    "\n",
    "            # Συνένωση της περιγραφής των προιόντων \n",
    "            description = ' '.join(product[\"description\"])\n",
    "\n",
    "            # String και Category στη λίστα με τα τελικά δεδομένα\n",
    "            valid_data.append((description, product[\"category\"][-1]))\n",
    "\n",
    "# Δημιουργία DataFrame\n",
    "df = pd.DataFrame(valid_data, columns=['Description', 'Category'])\n",
    "\n",
    "# Εκτύπωση των πρώτων 10 δεδομένων για έλεγχο\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f413430",
   "metadata": {},
   "source": [
    "# Βήμα 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cea6822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "[[1085   88]\n",
      " [ 354  439]]\n",
      "Accuracy: 0.775178026449644\n",
      "Precision: 0.7935064541183328\n",
      "Recall: 0.7392863170817974\n",
      "F1-Measure: 0.7479662629356352\n",
      "\n",
      "SVM:\n",
      "[[1082   91]\n",
      " [ 324  469]]\n",
      "Accuracy: 0.788911495422177\n",
      "Precision: 0.8035295163584637\n",
      "Recall: 0.7569230554220702\n",
      "F1-Measure: 0.7661795610518409\n",
      "\n",
      "Multi-Layer Perceptron:\n",
      "[[1057  116]\n",
      " [ 360  433]]\n",
      "Accuracy: 0.757884028484232\n",
      "Precision: 0.7673244353948219\n",
      "Recall: 0.7235680060718843\n",
      "F1-Measure: 0.7307608651870947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Διάβασμα δεδομένων απο json αρχείο και χωρισμός των κατηγοριών και περιγραφών σε δύο λίστες \n",
    "description = []\n",
    "categories = []\n",
    " \n",
    "with open('meta_Cell_Phones_and_Accessories.json') as f:\n",
    "    for line in f:\n",
    "        product = json.loads(line)\n",
    "        \n",
    "        if \"Chargers & Power Adapters\" in product[\"category\"] or \"Car Chargers\" in product[\"category\"]:\n",
    "            \n",
    "            if \"Chargers & Power Adapters\" in product[\"category\"][-1] or \"Car Chargers\" in product[\"category\"][-1]:\n",
    "\n",
    "                description.append(' '.join(product[\"description\"]))\n",
    "                categories.append(product[\"category\"][-1])\n",
    "\n",
    "\n",
    "# Αρχικοποίηση kfold και tf-idf \n",
    "vectorizer = TfidfVectorizer()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Αρχικοπο΄ίηση των τριών classifier\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "svm = SVC()\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "\n",
    "# Συνάρτηση που εκπαιδεύει εναν classifier και επιστρέφει τις επιθυμητές μετρικές\n",
    "def evaluate_classifier(classifier, x_train, y_train, x_test, y_test):\n",
    "    classifier.fit(x_train, y_train) # εκπαίδευση του εκάστοτε μοντέλου στο dataset των κατηγοριών και περιγραφών \n",
    "    y_prediction = classifier.predict(x_test) # η κατηγορία που προέβλεψε το μοντέλο για την περιγραφή που έλαβε ως input\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_prediction) # υπολογισμός του confusion matrix \n",
    "    accuracy = accuracy_score(y_test, y_prediction) # υπολογισμός της accuracy (μέση τιμή)\n",
    "    precision = precision_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του precision (μέση τιμή)\n",
    "    recall = recall_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του recall (μέση τιμή)\n",
    "    F1_measure = f1_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του f1 (μέση τιμή)\n",
    "   \n",
    "    return conf_matrix, accuracy, precision, recall, F1_measure\n",
    "\n",
    "kf_splits = list(kf.split(description)) # Αποθήκευση των splits του KFold\n",
    "\n",
    "for train_index, test_index in kf_splits:\n",
    "    \n",
    "    # Αποθήκευση των train και test υποσυνόλων του πεδίου description που φτιάχτηκαν με τη μεθοδο KFold\n",
    "    x_train = np.array(description)[train_index]\n",
    "    x_test = np.array(description)[test_index]\n",
    "    \n",
    "    # Αποθήκευση των train και test υποσυνόλων του πεδίου categories που φτιάχτηκαν με τη μεθοδο KFold\n",
    "    y_train = np.array(categories)[train_index]\n",
    "    y_test = np.array(categories)[test_index]\n",
    "    \n",
    "    # Εξαγωγή features με χρ΄ήση της tf-idf αναπαράστασης των περιγραφών \n",
    "    x_train_tfidf = vectorizer.fit_transform(x_train) \n",
    "    x_test_tfidf = vectorizer.transform(x_test)\n",
    "    \n",
    "    # Logistic Regression classifier \n",
    "    conf_matrix_logistic_regression, accuracy_logistic_regression, precision_logistic_regression, recall_logistic_regression, F1_measure_logistic_regression = evaluate_classifier(logistic_regression, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "\n",
    "    # SVM classifier \n",
    "    conf_matrix_svm, accuracy_svm, precision_svm, recall_svm, F1_measure_svm = evaluate_classifier(svm, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "    \n",
    "    # Multi-Layer Perceptron classifier \n",
    "    conf_matrix_mlp, accuracy_mlp, precision_mlp, recall_mlp, F1_measure_mlp = evaluate_classifier(mlp, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "    \n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για Logistic Regression classifier\n",
    "    print(\"Logistic Regression:\")\n",
    "    print(conf_matrix_logistic_regression)\n",
    "    print(\"Accuracy:\", accuracy_logistic_regression)\n",
    "    print(\"Precision:\", precision_logistic_regression)\n",
    "    print(\"Recall:\", recall_logistic_regression)\n",
    "    print(\"F1-Measure:\", F1_measure_logistic_regression)\n",
    "    print(\"\")\n",
    "\n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για SVM classifier\n",
    "    print(\"SVM:\")\n",
    "    print(conf_matrix_svm)\n",
    "    print(\"Accuracy:\", accuracy_svm)\n",
    "    print(\"Precision:\", precision_svm)\n",
    "    print(\"Recall:\", recall_svm)\n",
    "    print(\"F1-Measure:\", F1_measure_svm)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για Multi-Layer Perceptron classifier\n",
    "    print(\"Multi-Layer Perceptron:\")\n",
    "    print(conf_matrix_mlp)\n",
    "    print(\"Accuracy:\", accuracy_mlp)\n",
    "    print(\"Precision:\", precision_mlp)\n",
    "    print(\"Recall:\", recall_mlp)\n",
    "    print(\"F1-Measure:\", F1_measure_mlp)\n",
    "    print(\"\")\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7c283",
   "metadata": {},
   "source": [
    "Σχολιασμός: Από την παραπάνω εκτύπωση των αποτελεσμάτων, παρατηρούμε ότι υψηλότερο Accuracy (78.90%), Precision (80.35%), καθώς και Recall (75.70%), αλλά και F1 (0.7661) από τους τρεις classifiers έχει ο SVM. Συνεπώς, καταλήγουμε ότι ο SVM classifier είναι ο περισσότερο αποδοτικός για το παρόν πρόβλημα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2dcc7e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 20 λέξεις με το μεγαλύτερο θετικό βάρος: \n",
      "['wall', 'travel', 'ac', 'battery', 'home', '240v', 'desktop', 'dock', '60hz', 'qi', 'receiver', 'original', 'simply', '100', 'lg', 'pack', 'solar', 'of', 'sync', 'is']\n",
      "\n",
      "Logistic Regression 20 λέξεις με το μικρότερο αρνητικό βάρος: \n",
      "['car', 'vehicle', 'lighter', '12v', 'cigarette', '12', 'dc', '24v', 'dual', 'auto', 'indicated', '1a', 'retractable', 'powered', 'led', 'in', 'very', 'road', 'button', 'while']\n"
     ]
    }
   ],
   "source": [
    "# Εκπαίδευση του Logistic Regression στο τελευταίο fold\n",
    "\n",
    "# Αποθήκευση των train και test υποσυνόλων του πεδίου description που φτιάχτηκαν με τη μεθοδο KFold\n",
    "x_train = np.array(description)[train_index]\n",
    "x_test = np.array(description)[test_index]\n",
    "\n",
    "# Αποθήκευση των train και test υποσυνόλων του πεδίου categories που φτιάχτηκαν με τη μεθοδο KFold\n",
    "y_train = np.array(categories)[train_index] \n",
    "y_test = np.array(categories)[test_index]\n",
    "\n",
    "# Εξαγωγή features με χρ΄ήση της tf-idf αναπαράστασης των περιγραφών \n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer.transform(x_test)\n",
    "\n",
    "logistic_regression.fit(x_train_tfidf, y_train) # εκπαίδευση του μοντέλου\n",
    "\n",
    "# Εύρεση των 20 λέξεων με το μεγαλύτερο θετικό βάρος\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_positive_words = [feature_names[i] for i in logistic_regression.coef_[0].argsort()[-20:][::-1]] # παίρνω τις 20 λέξεις με μεγαλύτερο βάρος και τις ταξινομώ σε φθίνουσα σειρά (ο πίνακας coef περιέχει τα βάρη)\n",
    "\n",
    "# Εύρεση των 20 λέξεων με το μικρότερο αρνητικό βάρος\n",
    "top_negative_words = [feature_names[i] for i in logistic_regression.coef_[0].argsort()[:20]] # παίρνω τις 20 λέξεις με χαμηλότερο βάρος και τις ταξινομώ τα σε αύξουσα σειρά (ο πίνακας coef περιέχει τα βάρη)\n",
    "\n",
    "print(\"Logistic Regression 20 λέξεις με το μεγαλύτερο θετικό βάρος: \")\n",
    "print(top_positive_words)\n",
    "print(\"\")\n",
    "print(\"Logistic Regression 20 λέξεις με το μικρότερο αρνητικό βάρος: \")\n",
    "print(top_negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6ad2e",
   "metadata": {},
   "source": [
    "Οι λέξεις που έχουν το υψηλότερο θετικό βάρος στον Logistic Regression classifier αναφέρονται σε φορτιστές για πρίζες τοίχου (wall), σε φορητούς φορτιστές για ταξίδια (travel), σε φορτιστές που λειτουργούν με εναλλασσόμενο ρεύμα (ac), σε προιόντα που χρησιμοποιούν μπαταρίες (battery), σε πιθανώς προιόντα που προορίζονται για χρήση στο σπίτι (home), σε qi ασύρματη φόρτιση (qi), σε πιθανώς γνήσια προιόντα (original), σε σύνδεση προιόντων με φωτοβολταικά πάνελ (solar), κλπ.\n",
    "Στο αντίστοιχο αρνητικό βάρος, βλέπουμε ότι οι λέξεις αυτές είναι πιο γενικές και συνήθως δεν προσδιορίζουν τόσο το προϊόν (car, vehicle, lighter, 12v, cigarette, 12, dc, 24v, dual, auto, indicated, 1a, retractable, powered, led, in, very, road, button, while). Αυτές οι λέξεις είναι πιο γενικές και συναντώνται σε μεγαλύτερη ποικιλία προϊόντων και όχι τόσο πολύ στην κατηγορία που μας ενδιαφέρει."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90d2bf",
   "metadata": {},
   "source": [
    "# Βήμα 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "463491a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "[[1085   88]\n",
      " [ 354  439]]\n",
      "Accuracy: 0.775178026449644\n",
      "Precision: 0.7935064541183328\n",
      "Recall: 0.7392863170817974\n",
      "F1-Measure: 0.7479662629356352\n",
      "\n",
      "SVM:\n",
      "[[1082   91]\n",
      " [ 324  469]]\n",
      "Accuracy: 0.788911495422177\n",
      "Precision: 0.8035295163584637\n",
      "Recall: 0.7569230554220702\n",
      "F1-Measure: 0.7661795610518409\n",
      "\n",
      "Multi-Layer Perceptron:\n",
      "[[1046  127]\n",
      " [ 351  442]]\n",
      "Accuracy: 0.7568667344862665\n",
      "Precision: 0.7627743608259225\n",
      "Recall: 0.7245538272329601\n",
      "F1-Measure: 0.7315266516966923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "\n",
    "\n",
    "# Διάβασμα δεδομένων απο json αρχείο και χωρισμός των κατηγοριών και περιγραφών σε δύο λίστες \n",
    "description = []\n",
    "categories = []\n",
    " \n",
    "with open('meta_Cell_Phones_and_Accessories.json') as f:\n",
    "    for line in f:\n",
    "        product = json.loads(line)\n",
    "        \n",
    "        if \"Chargers & Power Adapters\" in product[\"category\"] or \"Car Chargers\" in product[\"category\"]:\n",
    "            \n",
    "            if \"Chargers & Power Adapters\" in product[\"category\"][-1] or \"Car Chargers\" in product[\"category\"][-1]:\n",
    "\n",
    "                description.append(' '.join(product[\"description\"]))\n",
    "                categories.append(product[\"category\"][-1])\n",
    "                \n",
    "# Αρχικοποίηση kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Αρχικοποίηση των τριών classifier\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "svm = SVC()\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "\n",
    "# Συνάρτηση που εκπαιδεύει εναν classifier και επιστρέφει τις επιθυμητές μετρικές\n",
    "def evaluate_classifier(classifier, x_train, y_train, x_test, y_test):\n",
    "    classifier.fit(x_train, y_train) # εκπαίδευση του εκάστοτε μοντέλου στο dataset των κατηγοριών και περιγραφών \n",
    "    y_prediction = classifier.predict(x_test) # η κατηγορία που προέβλεψε το μοντέλο για την περιγραφή που έλαβε ως input\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_prediction) # υπολογισμός του confusion matrix \n",
    "    accuracy = accuracy_score(y_test, y_prediction) # υπολογισμός της accuracy (μέση τιμή)\n",
    "    precision = precision_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του precision (μέση τιμή)\n",
    "    recall = recall_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του recall (μέση τιμή)\n",
    "    F1_measure = f1_score(y_test, y_prediction, average='macro', zero_division=1) # υπολογισμός του f1 (μέση τιμή)\n",
    "   \n",
    "    return conf_matrix, accuracy, precision, recall, F1_measure\n",
    "\n",
    "kf_splits = list(kf.split(description)) # Αποθήκευση των splits του KFold\n",
    "\n",
    "# Εκπαίδευση του Doc2Vec μοντέλου\n",
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(description)] # δημιουργία αντικειμένου TaggedDocument για κάθε αντικέιμενο στο description, τα οποία χωρίζονται με το κενό \n",
    "d2v_model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=4) # εκπαίδευση μοντέλου με δεδομένα από τη λίστα documents \n",
    "\n",
    "for train_index, test_index in kf_splits:\n",
    "    \n",
    "    # Αποθήκευση των train και test υποσυνόλων του πεδίου description που φτιάχτηκαν με τη μεθοδο KFold\n",
    "    x_train = np.array(description)[train_index] \n",
    "    x_test = np.array(description)[test_index]\n",
    "    \n",
    "    # Αποθήκευση των train και test υποσυνόλων του πεδίου categories που φτιάχτηκαν με τη μεθοδο KFold\n",
    "    y_train = np.array(categories)[train_index]\n",
    "    y_test = np.array(categories)[test_index]\n",
    "    \n",
    "    # Εξαγωγή των features μέσω embeddings\n",
    "    x_train_emb = [d2v_model.infer_vector(doc.split()) for doc in x_train]\n",
    "    x_test_emb = [d2v_model.infer_vector(doc.split()) for doc in x_test]\n",
    "    \n",
    "    # Logistic Regression classifier \n",
    "    conf_matrix_logistic_regression, accuracy_logistic_regression, precision_logistic_regression, recall_logistic_regression, F1_measure_logistic_regression = evaluate_classifier(logistic_regression, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "\n",
    "    # SVM classifier \n",
    "    conf_matrix_svm, accuracy_svm, precision_svm, recall_svm, F1_measure_svm = evaluate_classifier(svm, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "    \n",
    "    # Multi-Layer Perceptron classifier \n",
    "    conf_matrix_mlp, accuracy_mlp, precision_mlp, recall_mlp, F1_measure_mlp = evaluate_classifier(mlp, x_train_tfidf, y_train, x_test_tfidf, y_test)\n",
    "    \n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για Logistic Regression classifier\n",
    "    print(\"Logistic Regression:\")\n",
    "    print(conf_matrix_logistic_regression)\n",
    "    print(\"Accuracy:\", accuracy_logistic_regression)\n",
    "    print(\"Precision:\", precision_logistic_regression)\n",
    "    print(\"Recall:\", recall_logistic_regression)\n",
    "    print(\"F1-Measure:\", F1_measure_logistic_regression)\n",
    "    print(\"\")\n",
    "\n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για SVM classifier\n",
    "    print(\"SVM:\")\n",
    "    print(conf_matrix_svm)\n",
    "    print(\"Accuracy:\", accuracy_svm)\n",
    "    print(\"Precision:\", precision_svm)\n",
    "    print(\"Recall:\", recall_svm)\n",
    "    print(\"F1-Measure:\", F1_measure_svm)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Εκτύπωση confusion matrix και μετρικ΄ών για Multi-Layer Perceptron classifier\n",
    "    print(\"Multi-Layer Perceptron:\")\n",
    "    print(conf_matrix_mlp)\n",
    "    print(\"Accuracy:\", accuracy_mlp)\n",
    "    print(\"Precision:\", precision_mlp)\n",
    "    print(\"Recall:\", recall_mlp)\n",
    "    print(\"F1-Measure:\", F1_measure_mlp)\n",
    "    print(\"\")\n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e0ade",
   "metadata": {},
   "source": [
    "Και με τις δύο προσεγγίσεις τόσο με την TF-IDF, αλλά και με την Doc2Vec οι μετρικές της Logistic Regression και SVM παρέμειναν ίδιες. Ωστόσο, οι μετρικές του Multi-Layer Perceptron ήταν ελαφρώς χειρότερες με τη χρήση του Doc2Vec. Τέλος, τα αποτελέσματα στην Doc2Vec προσέγγιση μπορεί να είναι επηρεασμένα από την επιλογή των παραμέτρων (όπως το μέγεθος του vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bb6a5",
   "metadata": {},
   "source": [
    "# Βήμα 4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a930d7c",
   "metadata": {},
   "source": [
    "δεν επιλύθηκε, παρακάτω είναι κάποιες προσπάθειες"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84e4ad0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "C:\\Users\\ggian/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3007ac0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[-0.05419922  0.01708984 -0.00527954  0.33203125 -0.25       -0.01397705\n",
      " -0.15039062 -0.265625    0.01647949  0.3828125  -0.03295898 -0.09716797\n",
      " -0.16308594 -0.04443359  0.00946045  0.18457031  0.03637695  0.16601562\n",
      "  0.36328125 -0.25585938  0.375       0.171875    0.21386719 -0.19921875\n",
      "  0.13085938 -0.07275391 -0.02819824  0.11621094  0.15332031  0.09082031\n",
      "  0.06787109 -0.0300293  -0.16894531 -0.20800781 -0.03710938 -0.22753906\n",
      "  0.26367188  0.012146    0.18359375  0.31054688 -0.10791016 -0.19140625\n",
      "  0.21582031  0.13183594 -0.03515625  0.18554688 -0.30859375  0.04785156\n",
      " -0.10986328  0.14355469 -0.43554688 -0.0378418   0.10839844  0.140625\n",
      " -0.10595703  0.26171875 -0.17089844  0.39453125  0.12597656 -0.27734375\n",
      " -0.28125     0.14746094 -0.20996094  0.02355957  0.18457031  0.00445557\n",
      " -0.27929688 -0.03637695 -0.29296875  0.19628906  0.20703125  0.2890625\n",
      " -0.20507812  0.06787109 -0.43164062 -0.10986328 -0.2578125  -0.02331543\n",
      "  0.11328125  0.23144531 -0.04418945  0.10839844 -0.2890625  -0.09521484\n",
      " -0.10351562 -0.0324707   0.07763672 -0.13378906  0.22949219  0.06298828\n",
      "  0.08349609  0.02929688 -0.11474609  0.00534058 -0.12988281  0.02514648\n",
      "  0.08789062  0.24511719 -0.11474609 -0.296875   -0.59375    -0.29492188\n",
      " -0.13378906  0.27734375 -0.04174805  0.11621094  0.28320312  0.00241089\n",
      "  0.13867188 -0.00683594 -0.30078125  0.16210938  0.01171875 -0.13867188\n",
      "  0.48828125  0.02880859  0.02416992  0.04736328  0.05859375 -0.23828125\n",
      "  0.02758789  0.05981445 -0.03857422  0.06933594  0.14941406 -0.10888672\n",
      " -0.07324219  0.08789062  0.27148438  0.06591797 -0.37890625 -0.26171875\n",
      " -0.13183594  0.09570312 -0.3125      0.10205078  0.03063965  0.23632812\n",
      "  0.00582886  0.27734375  0.20507812 -0.17871094 -0.31445312 -0.01586914\n",
      "  0.13964844  0.13574219  0.0390625  -0.29296875  0.234375   -0.33984375\n",
      " -0.11816406  0.10644531 -0.18457031 -0.02099609  0.02563477  0.25390625\n",
      "  0.07275391  0.13574219 -0.00138092 -0.2578125  -0.2890625   0.10107422\n",
      "  0.19238281 -0.04882812  0.27929688 -0.3359375  -0.07373047  0.01879883\n",
      " -0.10986328 -0.04614258  0.15722656  0.06689453 -0.03417969  0.16308594\n",
      "  0.08642578  0.44726562  0.02026367 -0.01977539  0.07958984  0.17773438\n",
      " -0.04370117 -0.00952148  0.16503906  0.17285156  0.23144531 -0.04272461\n",
      "  0.02355957  0.18359375 -0.41601562 -0.01745605  0.16796875  0.04736328\n",
      "  0.14257812  0.08496094  0.33984375  0.1484375  -0.34375    -0.14160156\n",
      " -0.06835938 -0.14648438 -0.02844238  0.07421875 -0.07666016  0.12695312\n",
      "  0.05859375 -0.07568359 -0.03344727  0.23632812 -0.16308594  0.16503906\n",
      "  0.1484375  -0.2421875  -0.3515625  -0.30664062  0.00491333  0.17675781\n",
      "  0.46289062  0.14257812 -0.25       -0.25976562  0.04370117  0.34960938\n",
      "  0.05957031  0.07617188 -0.02868652 -0.09667969 -0.01281738  0.05859375\n",
      " -0.22949219 -0.1953125  -0.12207031  0.20117188 -0.42382812  0.06005859\n",
      "  0.50390625  0.20898438  0.11230469 -0.06054688  0.33203125  0.07421875\n",
      " -0.05786133  0.11083984 -0.06494141  0.05639648  0.01757812  0.08398438\n",
      "  0.13769531  0.2578125   0.16796875 -0.16894531  0.01794434  0.16015625\n",
      "  0.26171875  0.31640625 -0.24804688  0.05371094 -0.0859375   0.17089844\n",
      " -0.39453125 -0.00156403 -0.07324219 -0.04614258 -0.16210938 -0.15722656\n",
      "  0.21289062 -0.15820312  0.04394531  0.28515625  0.01196289 -0.26953125\n",
      " -0.04370117  0.37109375  0.04663086 -0.19726562  0.3046875  -0.36523438\n",
      " -0.23632812  0.08056641 -0.04248047 -0.14648438 -0.06225586 -0.0534668\n",
      " -0.05664062  0.18945312  0.37109375 -0.22070312  0.04638672  0.02612305\n",
      " -0.11474609  0.265625   -0.02453613  0.11083984 -0.02514648 -0.12060547\n",
      "  0.05297852  0.07128906  0.00063705 -0.36523438 -0.13769531 -0.12890625]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "#path = 'C:\\Users\\ggian/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz'\n",
    "g_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)  \n",
    "\n",
    "train_gmodel = [[g_model[x] for x in y if x in g_model] for y in x_train_gsim]\n",
    "train_data_labels = [(x,y) for (x,y) in zip(train_gmodel, y_train) if len(x) > 0]\n",
    "x_train_gm = [np.array(x) for (x,y) in train_data_labels]\n",
    "y_train_gm = [y for (x,y) in train_data_labels]\n",
    "\n",
    "X_train_gmodel = [x.mean(axis = 0) for x in x_train_gm]\n",
    "\n",
    "test_gmodel = [[g_model[x] for x in y if x in g_model] for y in x_test_gsim]\n",
    "test_data_labels = [(x,y) for (x,y) in zip(test_gmodel, y_test) if len(x) > 0]\n",
    "x_test_gm = [np.array(x) for (x,y) in test_data_labels]\n",
    "y_test_gm = [y for (x,y) in test_data_labels]\n",
    "\n",
    "x_test_gmodel = [x.mean(axis = 0) for x in x_test_gm]\n",
    "\n",
    "lr_clf.fit(X_news_train_gmodel, np.array(y_news_train_gm))\n",
    "lr_clf.score(np.array(X_news_test_gmodel),y_news_test_gm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df70445c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ggian\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\ggian\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7860,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m logistic_regression \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Εκπαίδευση του Logistic Regression classifier\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m logistic_regression\u001b[38;5;241m.\u001b[39mfit(x_train_emb_avg, y_train)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Πρόβλεψη των κατηγοριών για τα test δεδομένα\u001b[39;00m\n\u001b[0;32m     32\u001b[0m y_prediction \u001b[38;5;241m=\u001b[39m logistic_regression\u001b[38;5;241m.\u001b[39mpredict(x_test_emb_avg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7860,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Φόρτωση των GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# Εξαγωγή των embeddings για κάθε λέξη στο κάθε κείμενο\n",
    "x_train_emb = [[glove_model[word] for word in doc.split() if word in glove_model] for doc in x_train]\n",
    "x_test_emb = [[glove_model[word] for word in doc.split() if word in glove_model] for doc in x_test]\n",
    "\n",
    "# Υπολογισμός του μέσου όρου των embeddings για κάθε κείμενο\n",
    "x_train_emb_avg = [np.mean(embeddings, axis=0) for embeddings in x_train_emb]\n",
    "x_test_emb_avg = [np.mean(embeddings, axis=0) for embeddings in x_test_emb]\n",
    "\n",
    "\n",
    "# Αρχικοποίηση του Logistic Regression classifier\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Εκπαίδευση του Logistic Regression classifier\n",
    "logistic_regression.fit(x_train_emb_avg, y_train)\n",
    "\n",
    "# Πρόβλεψη των κατηγοριών για τα test δεδομένα\n",
    "y_prediction = logistic_regression.predict(x_test_emb_avg)\n",
    "\n",
    "# Υπολογισμός των μετρικών\n",
    "conf_matrix_logistic_regression = confusion_matrix(y_test, y_prediction)\n",
    "accuracy_logistic_regression = accuracy_score(y_test, y_prediction)\n",
    "precision_logistic_regression = precision_score(y_test, y_prediction, average='macro', zero_division=1)\n",
    "recall_logistic_regression = recall_score(y_test, y_prediction, average='macro', zero_division=1)\n",
    "F1_measure_logistic_regression = f1_score(y_test, y_prediction, average='macro', zero_division=1)\n",
    "\n",
    "# Εκτύπωση των μετρικών\n",
    "print(\"Logistic Regression:\")\n",
    "print(conf_matrix_logistic_regression)\n",
    "print(\"Accuracy:\", accuracy_logistic_regression)\n",
    "print(\"Precision:\", precision_logistic_regression)\n",
    "print(\"Recall:\", recall_logistic_regression)\n",
    "print(\"F1-Measure:\", F1_measure_logistic_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a51049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
